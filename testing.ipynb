{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5aa139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import cv2\n",
    "from mrcnn.visualize import display_instances\n",
    "import matplotlib.pyplot as plt\n",
    "import imgaug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bbe652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory of the project\n",
    "\n",
    "ROOT_DIR = r\"C:\\Users\\92321\\Threads of Fashion\"\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "\n",
    "# Path to trained weights file\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Directory to save logs and model checkpoints, if not provided\n",
    "# through the command line argument --logs\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9bfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConfig(Config):\n",
    "    \"\"\"Configuration for training on the custom  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"object\"\n",
    "\n",
    "\n",
    "    # NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.\n",
    "    GPU_COUNT = 1\n",
    "    \n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 4\n",
    "    \n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 11  # Background + Hard_hat, Safety_vest\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 2\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    \n",
    "    LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8b8a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n this augmentation is applied consecutively to each image. In other words, for each image, the augmentation apply flip LR,\\n and then followed by flip UD, then followed by rotation of -45 and 45, then followed by another rotation of -90 and 90,\\n and lastly followed by scaling with factor 0.5 and 1.5. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class CustomDataset(utils.Dataset):\n",
    "\n",
    "    def load_custom(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the Dog-Cat dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "      \n",
    "        self.add_class(\"object\", 1, \"Bridal Long Shirt\")\n",
    "        self.add_class(\"object\", 2, \"Bridal Dupatta\")\n",
    "        self.add_class(\"object\", 3, \"Bridal Choli\")\n",
    "        self.add_class(\"object\", 4, \"Bridal Lehenga\")\n",
    "        self.add_class(\"object\", 5,\"Wedding Guest Lehenga\")\n",
    "        self.add_class(\"object\", 6,\"Wedding Guest Dupatta\")\n",
    "        self.add_class(\"object\", 7,\"Wedding Guest Choli\")\n",
    "        self.add_class(\"object\", 8,\"Bridal Maxi\")\n",
    "        self.add_class(\"object\", 9,\"Wedding Guest Long Shirt\")\n",
    "        self.add_class(\"object\", 10,\"Bridal Peplum\")\n",
    "        self.add_class(\"object\", 11,\"Wedding Guest Maxi\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "     \n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        if subset == \"train\":\n",
    "            annotations1 = json.load(open(r'C:\\Users\\92321\\Threads of Fashion\\dataset\\train\\train.json'))\n",
    "        elif subset == \"val\":\n",
    "            annotations1 = json.load(open(r'C:\\Users\\92321\\Threads of Fashion\\dataset\\val\\val.json'))\n",
    "\n",
    "        annotations = list(annotations1.values())\n",
    "        print(annotations1)\n",
    "       \n",
    "          # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "        \n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "\n",
    "            polygons = [r['shape_attributes'] for r in a['regions'].values()] \n",
    "            objects = [s['region_attributes'] for s in a['regions'].values()]\n",
    "            print(\"objects:\",objects)\n",
    "            num_ids=[]\n",
    "            \n",
    "\n",
    "            for n in objects:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    if n['name']== \"Bridal Long Shirt\":\n",
    "                        num_ids.append(1)\n",
    "\n",
    "                    elif n['name']==\"Bridal Dupatta\":\n",
    "                         num_ids.append(2)\n",
    "                    \n",
    "                    elif n['name']==\"Bridal Choli\":\n",
    "                         num_ids.append(3)\n",
    "\n",
    "                    elif n['name']==\"Bridal Lehenga\":\n",
    "                         num_ids.append(4)\n",
    "                    \n",
    "                    elif n['name']==\"Wedding Guest Lehenga\":\n",
    "                         num_ids.append(5)\n",
    "                    \n",
    "                    elif n['name']==\"Wedding Guest Dupatta\":\n",
    "                         num_ids.append(6)\n",
    "\n",
    "                    elif n['name']==\"Wedding Guest Choli\":\n",
    "                         num_ids.append(7)\n",
    "                    \n",
    "                    elif n['name']==\"Bridal Maxi\":\n",
    "                         num_ids.append(8)\n",
    "\n",
    "                    elif n['name']==\"Wedding Guest Long Shirt\":\n",
    "                         num_ids.append(9)\n",
    "                    \n",
    "                    elif n['name']==\"Bridal Peplum\":\n",
    "                         num_ids.append(10)\n",
    "                    \n",
    "                    elif n['name']==\"Wedding Guest Maxi\":\n",
    "                         num_ids.append(11)\n",
    "                    \n",
    "                except:\n",
    "                    \n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            print(\"numids\",num_ids)\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"object\",  ## for a single class just add the name here\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons,\n",
    "                num_ids=num_ids\n",
    "                )\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a Dog-Cat dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"object\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] != \"object\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        num_ids = info['num_ids']\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "           \n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        # Map class names to class IDs.\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"object\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n",
    "def train(model):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Training dataset.\n",
    "    dataset_train = CustomDataset()\n",
    "    dataset_train.load_custom(r\"C:\\Users\\92321\\Threads of Fashion\\dataset\", \"train\")\n",
    "    dataset_train.prepare()\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = CustomDataset()\n",
    "    dataset_val.load_custom(r\"C:\\Users\\92321\\Threads of Fashion\\dataset\", \"val\")\n",
    "    dataset_val.prepare()\n",
    "\n",
    "    # *** This training schedule is an example. Update to your needs ***\n",
    "    # Since we're using a very small dataset, and starting from\n",
    "    # COCO trained weights, we don't need to train too long. Also,\n",
    "    # no need to train all layers, just the heads should do it.\n",
    "    \n",
    "    # print(\"Training network heads\")\n",
    "    # model.train(dataset_train, dataset_val,\n",
    "                # learning_rate=config.LEARNING_RATE,\n",
    "                # epochs=250,\n",
    "                # layers='heads')\n",
    "                \n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=2,\n",
    "                layers='heads', #layers='all', \n",
    "                augmentation = imgaug.augmenters.Sequential([ \n",
    "                imgaug.augmenters.Fliplr(0.5), \n",
    "                imgaug.augmenters.Flipud(0.5), \n",
    "                imgaug.augmenters.Affine(rotate=(-45, 45)), \n",
    "                imgaug.augmenters.Affine(rotate=(-90, 90)), \n",
    "                imgaug.augmenters.Affine(scale=(0.5, 1.5)),\n",
    "                imgaug.augmenters.Crop(px=(0, 10)),\n",
    "                imgaug.augmenters.Grayscale(alpha=(0.0, 1.0)),\n",
    "                imgaug.augmenters.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n",
    "                imgaug.augmenters.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                imgaug.augmenters.Invert(0.05, per_channel=True), # invert color channels\n",
    "                imgaug.augmenters.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                \n",
    "                ])\n",
    "                )\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    " this augmentation is applied consecutively to each image. In other words, for each image, the augmentation apply flip LR,\n",
    " and then followed by flip UD, then followed by rotation of -45 and 45, then followed by another rotation of -90 and 90,\n",
    " and lastly followed by scaling with factor 0.5 and 1.5. '''\n",
    "\t\n",
    "    \n",
    "# Another way of using imgaug    \n",
    "# augmentation = imgaug.Sometimes(5/6,aug.OneOf(\n",
    "                                            # [\n",
    "                                            # imgaug.augmenters.Fliplr(1), \n",
    "                                            # imgaug.augmenters.Flipud(1), \n",
    "                                            # imgaug.augmenters.Affine(rotate=(-45, 45)), \n",
    "                                            # imgaug.augmenters.Affine(rotate=(-90, 90)), \n",
    "                                            # imgaug.augmenters.Affine(scale=(0.5, 1.5))\n",
    "                                             # ]\n",
    "                                        # ) \n",
    "                                   # )\n",
    "                                    \n",
    "\n",
    "    \n",
    "\t\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2fb02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'120036931_372909770518626_6948827444533006010_n.jpg250199': {'fileref': '', 'size': 250199, 'filename': '120036931_372909770518626_6948827444533006010_n.jpg', 'base64_img_data': '', 'file_attributes': {}, 'regions': {'0': {'shape_attributes': {'name': 'polygon', 'all_points_x': [330, 330, 335, 381, 455, 517, 588, 571, 529, 514, 514, 522, 556, 593, 647, 593, 519, 494, 443, 379, 337, 106, 86, 69, 2, 2, 49, 106, 148, 221, 271, 362, 408, 413, 379, 308, 256, 204, 150, 89, 62, 44, 111, 177, 229, 276, 330], 'all_points_y': [273, 273, 271, 298, 271, 423, 534, 554, 541, 586, 649, 753, 859, 1046, 1304, 1316, 1316, 1311, 1326, 1289, 1314, 1242, 1171, 1201, 1191, 1031, 967, 932, 888, 755, 677, 608, 588, 566, 497, 394, 467, 534, 586, 649, 652, 642, 551, 463, 384, 295, 273]}, 'region_attributes': {'name': 'Bridal Long Shirt'}}, '1': {'shape_attributes': {'name': 'polygon', 'all_points_x': [465, 497, 539, 546, 541, 588, 588, 662, 755, 873, 947, 1075, 1075, 979, 923, 871, 832, 792, 709, 645, 578, 512, 465], 'all_points_y': [268, 258, 263, 322, 369, 463, 463, 522, 561, 625, 733, 886, 1279, 1220, 1117, 996, 851, 718, 603, 556, 514, 403, 268]}, 'region_attributes': {'name': 'Bridal Dupatta'}}}}, '120037318_184675689766761_6610925455850136981_n.jpg175040': {'fileref': '', 'size': 175040, 'filename': '120037318_184675689766761_6610925455850136981_n.jpg', 'base64_img_data': '', 'file_attributes': {}, 'regions': {'0': {'shape_attributes': {'name': 'polygon', 'all_points_x': [551, 576, 635, 696, 689, 593, 544, 539, 536, 507, 522, 472, 450, 445, 431, 504, 526, 551], 'all_points_y': [182, 187, 199, 285, 374, 367, 374, 391, 308, 362, 391, 438, 448, 408, 381, 229, 199, 182]}, 'region_attributes': {'name': 'Bridal Choli'}}, '1': {'shape_attributes': {'name': 'polygon', 'all_points_x': [551, 603, 649, 681, 728, 819, 866, 932, 1001, 1058, 996, 947, 927, 937, 878, 844, 785, 755, 617, 544, 502, 411, 396, 411, 357, 359, 381, 440, 455, 480, 514, 551, 566, 610, 627, 659, 684, 699, 706, 677, 696, 662, 686, 657, 677, 654, 667, 637, 654, 627, 652, 620, 640, 613, 632, 605, 622, 610, 551], 'all_points_y': [406, 423, 423, 413, 519, 691, 785, 918, 1100, 1240, 1269, 1287, 1287, 1304, 1314, 1299, 1321, 1321, 1333, 1321, 1326, 1299, 1284, 1272, 1272, 1235, 1196, 1188, 1159, 1183, 1161, 1171, 1151, 1166, 1137, 1149, 1114, 1090, 1055, 1036, 996, 974, 937, 903, 873, 839, 817, 782, 755, 731, 701, 664, 642, 605, 568, 536, 485, 450, 406]}, 'region_attributes': {'name': 'Bridal Lehenga'}}, '2': {'shape_attributes': {'name': 'polygon', 'all_points_x': [512, 502, 475, 467, 453, 460, 448, 453, 431, 413, 389, 335, 322, 367, 408, 408, 433, 458, 485, 522, 551, 571, 608, 627, 662, 699, 706, 674, 694, 659, 689, 659, 679, 657, 669, 635, 652, 627, 649, 620, 640, 610, 635, 608, 620, 613, 529, 504, 512], 'all_points_y': [411, 435, 522, 578, 617, 637, 667, 699, 797, 856, 984, 1134, 1166, 1188, 1186, 1169, 1176, 1154, 1183, 1159, 1164, 1149, 1166, 1132, 1146, 1087, 1060, 1041, 994, 979, 935, 905, 866, 832, 807, 785, 753, 726, 699, 667, 640, 603, 566, 536, 492, 453, 394, 426, 411]}, 'region_attributes': {'name': 'Bridal Dupatta'}}}}, '120043011_640276703344441_6546051516444075836_n.jpg163676': {'fileref': '', 'size': 163676, 'filename': '120043011_640276703344441_6546051516444075836_n.jpg', 'base64_img_data': '', 'file_attributes': {}, 'regions': {'0': {'shape_attributes': {'name': 'polygon', 'all_points_x': [458, 497, 529, 576, 586, 600, 608, 603, 617, 620, 637, 627, 652, 645, 664, 659, 681, 679, 785, 844, 1070, 1075, 376, 354, 379, 379, 374, 391, 376, 401, 384, 403, 384, 413, 396, 421, 416, 443, 421, 445, 440, 465, 453, 472, 475, 487, 472, 490, 482, 497, 487, 499, 480, 497, 477, 497, 480, 499, 487, 507, 526, 512, 502, 507, 492, 485, 458], 'all_points_y': [207, 189, 212, 396, 465, 504, 522, 556, 566, 600, 625, 657, 667, 711, 731, 753, 782, 804, 1038, 1129, 1218, 1343, 1348, 1321, 1309, 1309, 1294, 1284, 1274, 1262, 1245, 1233, 1223, 1210, 1186, 1169, 1149, 1117, 1100, 1073, 1048, 1016, 989, 957, 925, 861, 827, 807, 782, 748, 731, 696, 681, 649, 617, 600, 563, 534, 499, 477, 357, 320, 293, 278, 258, 221, 207]}, 'region_attributes': {'name': 'Bridal Dupatta'}}, '1': {'shape_attributes': {'name': 'polygon', 'all_points_x': [440, 401, 322, 234, 91, 106, 165, 162, 180, 209, 276, 300, 362, 374, 423, 408, 443, 416, 443, 440, 470, 453, 470, 490, 467, 485, 485, 485, 440], 'all_points_y': [406, 423, 608, 792, 1050, 1078, 1102, 1122, 1124, 1139, 1139, 1154, 1154, 1169, 1169, 1149, 1119, 1100, 1075, 1050, 1018, 984, 952, 859, 824, 804, 772, 470, 406]}, 'region_attributes': {'name': 'Bridal Lehenga'}}}}, '120135504_962866654196105_578232757815678181_n.jpg115286': {'fileref': '', 'size': 115286, 'filename': '120135504_962866654196105_578232757815678181_n.jpg', 'base64_img_data': '', 'file_attributes': {}, 'regions': {'0': {'shape_attributes': {'name': 'polygon', 'all_points_x': [438, 571, 642, 713, 728, 743, 733, 765, 755, 790, 782, 814, 814, 895, 531, 487, 480, 502, 531, 492, 490, 494, 524, 526, 514, 512, 539, 524, 524, 539, 514, 504, 529, 494, 482, 509, 480, 460, 482, 492, 443, 423, 421, 438], 'all_points_y': [362, 335, 389, 622, 669, 684, 738, 785, 854, 893, 969, 1014, 1092, 1343, 1346, 1292, 1257, 1225, 1210, 1188, 1142, 1114, 1080, 1082, 1028, 982, 952, 915, 864, 822, 770, 728, 689, 659, 613, 568, 531, 485, 450, 445, 423, 411, 381, 362]}, 'region_attributes': {'name': 'Bridal Dupatta'}}}}}\n",
      "objects: [{'name': 'Bridal Long Shirt'}, {'name': 'Bridal Dupatta'}]\n",
      "numids [1, 2]\n",
      "objects: [{'name': 'Bridal Choli'}, {'name': 'Bridal Lehenga'}, {'name': 'Bridal Dupatta'}]\n",
      "numids [3, 4, 2]\n",
      "objects: [{'name': 'Bridal Dupatta'}, {'name': 'Bridal Lehenga'}]\n",
      "numids [2, 4]\n",
      "objects: [{'name': 'Bridal Dupatta'}]\n",
      "numids [2]\n",
      "{'120202576_369061330916954_7997385290155786997_n.jpg145952': {'fileref': '', 'size': 145952, 'filename': '120202576_369061330916954_7997385290155786997_n.jpg', 'base64_img_data': '', 'file_attributes': {}, 'regions': {'0': {'shape_attributes': {'name': 'polygon', 'all_points_x': [490, 507, 524, 556, 595, 598, 581, 583, 566, 637, 664, 686, 701, 691, 605, 566, 600, 620, 647, 620, 578, 544, 514, 470, 438, 406, 372, 349, 308, 376, 435, 487, 497, 448, 443, 448, 448, 460, 490], 'all_points_y': [224, 259, 279, 279, 266, 266, 387, 470, 566, 409, 296, 232, 249, 431, 603, 566, 665, 871, 1058, 1068, 1051, 1068, 1051, 1066, 1046, 1063, 1046, 1071, 1044, 795, 617, 507, 493, 458, 438, 423, 414, 244, 224]}, 'region_attributes': {'name': 'Bridal Long Shirt'}}, '1': {'shape_attributes': {'name': 'polygon', 'all_points_x': [674, 679, 652, 625, 684, 743, 881, 942, 925, 895, 908, 888, 866, 839, 817, 768, 716, 701, 667, 652, 600, 586, 608, 627, 674], 'all_points_y': [207, 219, 325, 428, 522, 677, 1092, 1213, 1237, 1225, 1257, 1269, 1260, 1274, 1265, 1282, 1262, 1272, 1198, 1068, 595, 371, 231, 197, 207]}, 'region_attributes': {'name': 'Bridal Dupatta'}}, '2': {'shape_attributes': {'name': 'polygon', 'all_points_x': [477, 448, 416, 347, 263, 224, 236, 231, 315, 359, 401, 411, 480, 652, 699, 723, 800, 861, 918, 908, 942, 950, 984, 772, 674, 477], 'all_points_y': [490, 549, 630, 782, 1001, 1129, 1213, 1252, 1277, 1301, 1309, 1301, 1333, 1333, 1321, 1326, 1277, 1277, 1262, 1237, 1235, 1205, 1198, 689, 487, 490]}, 'region_attributes': {'name': 'Bridal Lehenga'}}}}, '120338812_340594057052396_7202063582974370592_n.jpg353354': {'fileref': '', 'size': 353354, 'filename': '120338812_340594057052396_7202063582974370592_n.jpg', 'base64_img_data': '', 'file_attributes': {}, 'regions': {'0': {'shape_attributes': {'name': 'polygon', 'all_points_x': [842, 879, 925, 953, 1026, 1060, 1109, 1069, 1011, 956, 913, 1048, 999, 980, 993, 940, 897, 836, 799, 814, 793, 728, 848, 863, 796, 710, 685, 688, 688, 737, 796, 842], 'all_points_y': [313, 341, 338, 313, 335, 384, 562, 817, 802, 737, 676, 581, 433, 525, 572, 565, 572, 578, 452, 378, 353, 568, 645, 661, 741, 811, 787, 578, 535, 344, 329, 313]}, 'region_attributes': {'name': 'Bridal Choli'}}, '1': {'shape_attributes': {'name': 'polygon', 'all_points_x': [722, 793, 796, 842, 845, 860, 879, 750, 510, 529, 495, 602, 664, 691, 741, 722], 'all_points_y': [353, 335, 406, 624, 827, 1205, 1567, 1595, 1521, 1487, 1466, 1051, 848, 774, 568, 353]}, 'region_attributes': {'name': 'Bridal Dupatta'}}, '2': {'shape_attributes': {'name': 'polygon', 'all_points_x': [851, 891, 983, 1115, 1266, 1257, 1294, 1278, 1223, 1220, 1106, 1014, 934, 824, 799, 796, 882, 851], 'all_points_y': [612, 618, 608, 962, 1386, 1407, 1512, 1546, 1573, 1601, 1607, 1641, 1641, 1619, 1622, 1579, 1564, 612]}, 'region_attributes': {'name': 'Bridal Lehenga'}}}}, '120434039_214176670038324_7059549794799278494_n.jpg420479': {'fileref': '', 'size': 420479, 'filename': '120434039_214176670038324_7059549794799278494_n.jpg', 'base64_img_data': '', 'file_attributes': {}, 'regions': {'0': {'shape_attributes': {'name': 'polygon', 'all_points_x': [565, 504, 455, 489, 525, 547, 544, 529, 688, 744, 799, 820, 827, 827, 839, 931, 1177, 1334, 1346, 1091, 1002, 1036, 848, 777, 685, 655, 565], 'all_points_y': [519, 538, 698, 756, 633, 716, 863, 977, 814, 771, 814, 907, 1076, 1410, 1457, 1429, 1294, 1109, 879, 827, 839, 790, 753, 556, 513, 535, 519]}, 'region_attributes': {'name': 'Bridal Long Shirt'}}, '1': {'shape_attributes': {'name': 'polygon', 'all_points_x': [273, 452, 492, 476, 495, 461, 486, 458, 482, 461, 482, 473, 470, 532, 747, 793, 820, 827, 867, 1014, 6, 6, 46, 141, 221, 273, 304, 273], 'all_points_y': [790, 701, 771, 814, 851, 888, 919, 974, 1011, 1054, 1082, 1125, 1180, 974, 781, 820, 934, 1438, 1540, 1681, 1681, 1420, 1386, 1337, 1168, 996, 860, 790]}, 'region_attributes': {'name': 'Bridal Dupatta'}}}}, '120555343_791325278328246_7044954838078495797_n.jpg123557': {'fileref': '', 'size': 123557, 'filename': '120555343_791325278328246_7044954838078495797_n.jpg', 'base64_img_data': '', 'file_attributes': {}, 'regions': {'0': {'shape_attributes': {'name': 'polygon', 'all_points_x': [515, 478, 427, 401, 380, 364, 313, 329, 345, 427, 434, 464, 520, 553, 564, 532, 515], 'all_points_y': [191, 175, 189, 201, 334, 411, 527, 546, 539, 380, 366, 376, 383, 329, 236, 189, 191]}, 'region_attributes': {'name': 'Wedding Guest Choli'}}, '1': {'shape_attributes': {'name': 'polygon', 'all_points_x': [422, 448, 476, 511, 539, 555, 574, 581, 574, 590, 588, 588, 567, 515, 471, 329, 322, 252, 233, 168, 159, 203, 287, 336, 422], 'all_points_y': [390, 394, 406, 392, 450, 518, 562, 616, 695, 718, 802, 891, 982, 1054, 1155, 1136, 1155, 1143, 1085, 1078, 1059, 921, 735, 581, 390]}, 'region_attributes': {'name': 'Wedding Guest Lehenga'}}, '2': {'shape_attributes': {'name': 'polygon', 'all_points_x': [520, 550, 555, 595, 602, 620, 595, 625, 625, 611, 637, 637, 625, 683, 702, 672, 711, 704, 686, 693, 716, 702, 728, 709, 730, 716, 763, 760, 718, 772, 807, 812, 812, 854, 868, 844, 882, 954, 949, 884, 823, 753, 781, 725, 658, 634, 590, 553, 481, 467, 539, 569, 597, 597, 578, 588, 557, 518, 541, 546, 520], 'all_points_y': [383, 378, 406, 443, 467, 483, 509, 509, 534, 550, 571, 595, 595, 644, 681, 709, 721, 751, 746, 767, 791, 805, 835, 844, 863, 893, 924, 966, 984, 1045, 1050, 1057, 1068, 1071, 1089, 1089, 1113, 1152, 1197, 1201, 1234, 1171, 1234, 1250, 1234, 1274, 1239, 1262, 1243, 1159, 1031, 991, 893, 721, 690, 597, 490, 394, 378, 380, 383]}, 'region_attributes': {'name': 'Wedding Guest Dupatta'}}}}}\n",
      "objects: [{'name': 'Bridal Long Shirt'}, {'name': 'Bridal Dupatta'}, {'name': 'Bridal Lehenga'}]\n",
      "numids [1, 2, 4]\n",
      "objects: [{'name': 'Bridal Choli'}, {'name': 'Bridal Dupatta'}, {'name': 'Bridal Lehenga'}]\n",
      "numids [3, 2, 4]\n",
      "objects: [{'name': 'Bridal Long Shirt'}, {'name': 'Bridal Dupatta'}]\n",
      "numids [1, 2]\n",
      "objects: [{'name': 'Wedding Guest Choli'}, {'name': 'Wedding Guest Lehenga'}, {'name': 'Wedding Guest Dupatta'}]\n",
      "numids [7, 5, 6]\n",
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\92321\\Threads of Fashion\\logs\\object20240106T0537\\mask_rcnn_object_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/GatherV2_2:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/GatherV2_5:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/GatherV2_8:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/GatherV2_11:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_classifier_1/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/GatherV2_2:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/Shape:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/GatherV2_5:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/Shape_1:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/GatherV2_8:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/GatherV2_11:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/roi_align_mask_1/concat_grad/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_4_grad/Reshape_1:0\", shape=(6000,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_4_grad/Reshape:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_4_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_5_grad/Reshape_1:0\", shape=(6000,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_5_grad/Reshape:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_5_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_6_grad/Reshape_1:0\", shape=(6000,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_6_grad/Reshape:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_6_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_7_grad/Reshape_1:0\", shape=(6000,), dtype=int32), values=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_7_grad/Reshape:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"training/SGD/gradients/gradients/ROI_1/GatherV2_7_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - batch: 0.5000 - size: 4.0000 - loss: 6.3340 - rpn_class_loss: 0.0342 - rpn_bbox_loss: 1.1226 - mrcnn_class_loss: 2.2658 - mrcnn_bbox_loss: 1.3805 - mrcnn_mask_loss: 1.5310      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\92321\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 542s 420s/step - batch: 0.5000 - size: 4.0000 - loss: 6.3340 - rpn_class_loss: 0.0342 - rpn_bbox_loss: 1.1226 - mrcnn_class_loss: 2.2658 - mrcnn_bbox_loss: 1.3805 - mrcnn_mask_loss: 1.5310 - val_loss: 4.0436 - val_rpn_class_loss: 0.0177 - val_rpn_bbox_loss: 0.6079 - val_mrcnn_class_loss: 1.0653 - val_mrcnn_bbox_loss: 1.2832 - val_mrcnn_mask_loss: 1.0696\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 507s 403s/step - batch: 0.5000 - size: 4.0000 - loss: 3.8388 - rpn_class_loss: 0.0392 - rpn_bbox_loss: 0.9867 - mrcnn_class_loss: 0.6317 - mrcnn_bbox_loss: 1.1244 - mrcnn_mask_loss: 1.0568 - val_loss: 3.3941 - val_rpn_class_loss: 0.0172 - val_rpn_bbox_loss: 0.6263 - val_mrcnn_class_loss: 0.7069 - val_mrcnn_bbox_loss: 1.1380 - val_mrcnn_mask_loss: 0.9057\n"
     ]
    }
   ],
   "source": [
    "config = CustomConfig()\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                                  model_dir=DEFAULT_LOGS_DIR)\n",
    "\n",
    "weights_path = COCO_WEIGHTS_PATH\n",
    "        # Download weights file\n",
    "if not os.path.exists(weights_path):\n",
    "  utils.download_trained_weights(weights_path)\n",
    "\n",
    "model.load_weights(weights_path, by_name=True, exclude=[\n",
    "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "train(model)\t\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
